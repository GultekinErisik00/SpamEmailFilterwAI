{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be7468cd",
   "metadata": {},
   "source": [
    "Gerekli kütüphaneleri yükleme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1098e893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gultekinqwe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gultekinqwe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Gerekli kütüphaneler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e3d53d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setlerini oku\n",
    "d1 = pd.read_csv(\"dataFile/spam_modified.csv\", sep=';', header=0, usecols=[0, 1])\n",
    "d2 = pd.read_csv(\"dataFile/enron_spam_data_modified.csv\", sep=';', header=0, usecols=[0, 1])\n",
    "\n",
    "#sütunlara etiket ve metin şeklinde isimlendirme yapılır\n",
    "d1.columns = ['label', 'text']\n",
    "d2.columns = ['label', 'text']\n",
    "# ham 0, spam 1 şeklinde sütuna dönüşüm yapılır\n",
    "d1['label'] = d1['label'].str.strip().str.lower().map({'ham': 0, 'spam': 1})\n",
    "d2['label'] = d2['label'].str.strip().str.lower().map({'ham': 0, 'spam': 1})\n",
    "\n",
    "#Boş satır ve sütunlar silinir    \n",
    "d1.dropna(subset=['label', 'text'], inplace=True)\n",
    "d2.dropna(subset=['label', 'text'], inplace=True)\n",
    "\n",
    "#Float değerler oluşmuşsa bunları integer a çevirir\n",
    "d1['label'] = d1['label'].astype(int)\n",
    "d2['label'] = d2['label'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4273d3d",
   "metadata": {},
   "source": [
    "Dataların ön işleme yapıldığı fonksiyon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26c78cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ön işleme fonksiyonu \n",
    "# Küçük harfe dönüştürme\n",
    "#Noktalama işaretlerinin kaldırılması\n",
    "#Stop-word’lerin çıkarılması\n",
    "#Lemmatization (tercihen)\n",
    "#Yinelenen e-postaların temizlenmesi\n",
    "def preprocess(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-zA-Z ]\", \"\", text)\n",
    "    \n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259d1366",
   "metadata": {},
   "source": [
    "Özellik çıkarımının kullanıldığı fonksiyon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fa21b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Özellik çıkarımı fonksiyonu\n",
    "def extract_features(data):\n",
    "    data[\"clean_text\"] = data[\"text\"].apply(preprocess)\n",
    "    data = data[data[\"clean_text\"].str.strip() != \"\"]\n",
    "    data = data[data[\"clean_text\"].str.len() > 3]\n",
    "\n",
    "    #TF-IDF vektörlerinin oluştuğu kısım (Her kelimenin sayısal önemini gösteren vektörler üretir)\n",
    "    tfidf = TfidfVectorizer(max_features=5000)#maximum 5000 kelimeye kadar dikkate alınır\n",
    "    tfidf_vectors = tfidf.fit_transform(data[\"clean_text\"])\n",
    "\n",
    "    #LSA kullanıldığı kısım (TF-IDF den gelen matrisin boyutu düşürülür) 100 öznitelik vektörü üretilir\n",
    "    lsa = TruncatedSVD(n_components=100, random_state=42)\n",
    "    X_lsa = lsa.fit_transform(tfidf_vectors)\n",
    "    \n",
    "    \n",
    "    # Burda temizlenmiş metinlerden gelen kelimeler benzerliklerine göre benzer vektörlere ayrılır\n",
    "    sentences = [text.split() for text in data[\"clean_text\"]]\n",
    "    w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2)\n",
    "    word_vectors = w2v_model.wv\n",
    "\n",
    "    #Word2Vec ile elde edilen kelime vektörleri, 50 kümeye ayrılır. Her küme belirli bir anlam grubunu temsil eder.\n",
    "    kmeans = KMeans(n_clusters=50, random_state=42)\n",
    "    kmeans.fit(word_vectors.vectors.astype(np.float64))\n",
    "\n",
    "    #Her bir metindeki kelimeleri, Word2Vec vektörleri üzerinden KMeans ile oluşturulmuş 50 kümeye atar ve Hangi kümeden kaç kelime geçtiğini sayar.\n",
    "    #Bu sayım üzerinden 50 boyutlu öznitelik vektörü üretir.\n",
    "    def get_cluster_features(text):\n",
    "        cluster_count = np.zeros(50)\n",
    "        if not isinstance(text, str): return cluster_count\n",
    "        for word in text.split():\n",
    "            if word in word_vectors:\n",
    "                vec = np.asarray(word_vectors[word], dtype=np.float64)\n",
    "                idx = kmeans.predict([vec])[0]\n",
    "                cluster_count[idx] += 1\n",
    "        return cluster_count\n",
    "    # get_cluster_features fonksiyonu, her metni 50 boyutlu bir vektöre dönüştürür (kelime kümelerine göre)\n",
    "    semantic_vectors = np.array([get_cluster_features(text) for text in data[\"clean_text\"]])\n",
    "    # LSA (100 boyut) ve semantic_vectors (50 boyut) birleştirilerek toplam 150 boyutlu öznitelik vektörü elde edilir\n",
    "    X = np.hstack((X_lsa, semantic_vectors))\n",
    "    #Etiket vektörü\n",
    "    y = data[\"label\"].values\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da3bf9",
   "metadata": {},
   "source": [
    "Modellerin eğitildiği fonksiyon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd27a2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "######1. DATA SETI ICIN Grid search algoritmasiyla uygun olan parametre secilicek cunku dosya boyutu 2. veri setine gore daha az########\n",
    "# Eğitim/test ayrımı \n",
    "X1, y1 = extract_features(d1)  \n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, stratify=y1, random_state=42)\n",
    "\n",
    "# SVM için Grid Search parametreleri\n",
    "svm_params = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"kernel\": [\"linear\", \"rbf\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "grid_svm = GridSearchCV(SVC(probability=True), svm_params, cv=2, scoring='f1')\n",
    "grid_svm.fit(X_train1, y_train1)\n",
    "\n",
    "# Random Forest için Grid Search parametreleri\n",
    "rf_params = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [None, 10, 20]\n",
    "}\n",
    "#RandomForest icin gridSearch yapiliyor\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(), rf_params, cv=2, scoring='f1')\n",
    "grid_rf.fit(X_train1, y_train1)\n",
    "\n",
    "# MLP için Grid Search parametreleri\n",
    "mlp_params = {\n",
    "    \"hidden_layer_sizes\": [(100,), (50, 50)],\n",
    "    \"learning_rate_init\": [0.001, 0.01],\n",
    "    \"max_iter\": [200]\n",
    "}\n",
    "#NLP icin GridSearch yapiliyor\n",
    "grid_mlp = GridSearchCV(MLPClassifier(), mlp_params, cv=2, scoring='f1')\n",
    "grid_mlp.fit(X_train1, y_train1)\n",
    "\n",
    "# Test edilen modeller en iyi parametreleriyle beraber listeye ekleniyor\n",
    "best_models_D1 = {\n",
    "    \"SVM\": SVC(**grid_svm.best_params_, probability=True),\n",
    "    \"Random Forest\": RandomForestClassifier(**grid_rf.best_params_),\n",
    "    \"MLP\": MLPClassifier(**grid_mlp.best_params_)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce7afe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X2, y2 = extract_features(d2)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, stratify=y2, random_state=42)\n",
    "\n",
    "# Randomized Search Parametreler (optimize edilmiş)\n",
    "svm_params2 = {\n",
    "    \"C\": [0.1, 1],\n",
    "    \"kernel\": [\"linear\"],\n",
    "    \"gamma\": [\"scale\"]\n",
    "}\n",
    "rf_params2 = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [10]\n",
    "}\n",
    "mlp_params2 = {\n",
    "    \"hidden_layer_sizes\": [(100,), (50,)],\n",
    "    \"learning_rate_init\": [0.001],\n",
    "    \"max_iter\": [200]\n",
    "}\n",
    "\n",
    "# RandomizedSearch \n",
    "random_svm = RandomizedSearchCV(SVC(probability=True), svm_params2, n_iter=5, cv=2, scoring='f1', random_state=42, n_jobs=-1)\n",
    "random_rf  = RandomizedSearchCV(RandomForestClassifier(), rf_params2, n_iter=5, cv=2, scoring='f1', random_state=42, n_jobs=-1)\n",
    "random_mlp = RandomizedSearchCV(MLPClassifier(), mlp_params2, n_iter=5, cv=2, scoring='f1', random_state=42, n_jobs=-1)\n",
    "\n",
    "\n",
    "# \n",
    "random_svm.fit(X_train2, y_train2)\n",
    "random_rf.fit(X_train2, y_train2)\n",
    "random_mlp.fit(X_train2, y_train2)\n",
    "\n",
    "# Test edilen modellerin en iyi parametreleriyle beraber listeye ekleniyor\n",
    "best_models_D2 = {\n",
    "    \"SVM\": SVC(**random_svm.best_params_, probability=True),\n",
    "    \"Random Forest\": RandomForestClassifier(**random_rf.best_params_),\n",
    "    \"MLP\": MLPClassifier(**random_mlp.best_params_)\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6f647a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(X_train, y_train, X_test, y_test, models, dataSet_name):\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"\\n--- {dataSet_name} | {name} ---\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "        print(\"ROC AUC:\", roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "155a3fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Spam Modified Dataset | SVM ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       956\n",
      "           1       0.96      0.86      0.91       149\n",
      "\n",
      "    accuracy                           0.98      1105\n",
      "   macro avg       0.97      0.93      0.95      1105\n",
      "weighted avg       0.98      0.98      0.98      1105\n",
      "\n",
      "Accuracy: 0.9764705882352941\n",
      "F1 Score: 0.9078014184397163\n",
      "ROC AUC: 0.9701286119457471\n",
      "\n",
      "--- Spam Modified Dataset | Random Forest ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       956\n",
      "           1       0.99      0.82      0.90       149\n",
      "\n",
      "    accuracy                           0.97      1105\n",
      "   macro avg       0.98      0.91      0.94      1105\n",
      "weighted avg       0.98      0.97      0.97      1105\n",
      "\n",
      "Accuracy: 0.9746606334841629\n",
      "F1 Score: 0.8970588235294118\n",
      "ROC AUC: 0.9660322653112803\n",
      "\n",
      "--- Spam Modified Dataset | MLP ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       956\n",
      "           1       0.98      0.85      0.91       149\n",
      "\n",
      "    accuracy                           0.98      1105\n",
      "   macro avg       0.98      0.92      0.95      1105\n",
      "weighted avg       0.98      0.98      0.98      1105\n",
      "\n",
      "Accuracy: 0.9773755656108597\n",
      "F1 Score: 0.9097472924187726\n",
      "ROC AUC: 0.9712659009856505\n",
      "\n",
      "--- Enron Dataset | SVM ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      3296\n",
      "           1       0.96      0.99      0.97      3362\n",
      "\n",
      "    accuracy                           0.97      6658\n",
      "   macro avg       0.97      0.97      0.97      6658\n",
      "weighted avg       0.97      0.97      0.97      6658\n",
      "\n",
      "Accuracy: 0.9738660258335836\n",
      "F1 Score: 0.9745092294169353\n",
      "ROC AUC: 0.9938223480735578\n",
      "\n",
      "--- Enron Dataset | Random Forest ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      3296\n",
      "           1       0.96      0.99      0.98      3362\n",
      "\n",
      "    accuracy                           0.97      6658\n",
      "   macro avg       0.98      0.97      0.97      6658\n",
      "weighted avg       0.98      0.97      0.97      6658\n",
      "\n",
      "Accuracy: 0.9746170021027335\n",
      "F1 Score: 0.9752815562381162\n",
      "ROC AUC: 0.9967234002385311\n",
      "\n",
      "--- Enron Dataset | MLP ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      3296\n",
      "           1       0.98      0.99      0.98      3362\n",
      "\n",
      "    accuracy                           0.98      6658\n",
      "   macro avg       0.98      0.98      0.98      6658\n",
      "weighted avg       0.98      0.98      0.98      6658\n",
      "\n",
      "Accuracy: 0.9830279363172124\n",
      "F1 Score: 0.9832716506291636\n",
      "ROC AUC: 0.9964293874860666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results(X_train1, y_train1, X_test1, y_test1, best_models_D1, \"Spam Modified Dataset\")\n",
    "\n",
    "results(X_train2, y_train2, X_test2, y_test2, best_models_D2, \"Enron Dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
