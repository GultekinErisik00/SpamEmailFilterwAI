{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.7.0)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/gultekinqwe/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/gultekinqwe/Library/Python/3.11/lib/python/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (7.3.0.post1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/gultekinqwe/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas numpy matplotlib seaborn scikit-learn nltk gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gultekinqwe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gultekinqwe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label değerleri: ['ham' 'spam']\n",
      "Toplam birleşik veri şekli: (38916, 2)\n",
      "Etiket dağılımı:\n",
      " label\n",
      "0    21317\n",
      "1    17599\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# df1 = pd.read_csv(\"dataFile/spam_modified.csv\", sep=';', names=['label', 'text'], usecols=[0, 1])\n",
    "# df2 = pd.read_csv(\"dataFile/enron_spam_data_modified.csv\", sep=';', names=['label', 'text'], usecols=[0, 1])\n",
    "\n",
    "# # Boşlukları sil, küçük harfe çevir\n",
    "# df1['label'] = df1['label'].str.strip().str.lower()\n",
    "# df2['label'] = df2['label'].str.strip().str.lower()\n",
    "\n",
    "# # Etiketleri doğru eşle\n",
    "# df1['label'] = df1['label'].map({'ham': 0, 'spam': 1}).fillna(-1).astype(int)\n",
    "# df2['label'] = df2['label'].map({'ham': 0, 'spam': 1}).fillna(-1).astype(int)\n",
    "# df1 = df1[df1[\"label\"] != -1]\n",
    "# df2 = df2[df2[\"label\"] != -1]\n",
    "\n",
    "\n",
    "# # NaN varsa temizle\n",
    "# df1.dropna(inplace=True)\n",
    "# df2.dropna(inplace=True)\n",
    "\n",
    "# #birlestirme\n",
    "# df = pd.concat([df2, df1], ignore_index=True)\n",
    "\n",
    "# print(\"Toplam birleşik veri şekli:\", df.shape)\n",
    "# print(\"Etiket dağılımı:\\n\", df[\"label\"].value_counts())\n",
    "\n",
    "# # Son halini gör\n",
    "# df.head()\n",
    "\n",
    "df1 = pd.read_csv(\"dataFile/spam_modified.csv\", sep=';', header=0, usecols=[0, 1])\n",
    "df2 = pd.read_csv(\"dataFile/enron_spam_data_modified.csv\", sep=';', header=0, usecols=[0, 1])\n",
    "\n",
    "df1.columns = ['label', 'text']\n",
    "df2.columns = ['label', 'text']\n",
    "\n",
    "df1['label'] = df1['label'].str.strip().str.lower()\n",
    "df2['label'] = df2['label'].str.strip().str.lower()\n",
    "\n",
    "print(\"Label değerleri:\", df2['label'].unique())\n",
    "\n",
    "df1['label'] = df1['label'].map({'ham': 0, 'spam': 1})\n",
    "df2['label'] = df2['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "df1.dropna(subset=['label', 'text'], inplace=True)\n",
    "df2.dropna(subset=['label', 'text'], inplace=True)\n",
    "\n",
    "df1['label'] = df1['label'].astype(int)\n",
    "df2['label'] = df2['label'].astype(int)\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "print(\"Toplam birleşik veri şekli:\", df.shape)\n",
    "print(\"Etiket dağılımı:\\n\", df[\"label\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5571, 2)\n",
      "(33345, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df1.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gultekinqwe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gultekinqwe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>freemsg hey darling week word back id like fun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      0                      Ok lar... Joking wif u oni...   \n",
       "1      1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2      0  U dun say so early hor... U c already then say...   \n",
       "3      0  Nah I don't think he goes to usf, he lives aro...   \n",
       "4      1  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "\n",
       "                                          clean_text  \n",
       "0                            ok lar joking wif u oni  \n",
       "1  free entry wkly comp win fa cup final tkts st ...  \n",
       "2                u dun say early hor u c already say  \n",
       "3           nah dont think go usf life around though  \n",
       "4  freemsg hey darling week word back id like fun...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(preprocess)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38916, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), max_features=3000)\n",
    "X_tfidf = tfidf.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "lsa = TruncatedSVD(n_components=300, random_state=42)\n",
    "X_lsa = lsa.fit_transform(X_tfidf)\n",
    "X_lsa.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38916, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "sentences = [text.split() for text in df[\"clean_text\"]]\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2)\n",
    "word_vectors = w2v_model.wv\n",
    "\n",
    "kmeans = KMeans(n_clusters=50, random_state=42)\n",
    "kmeans.fit(word_vectors.vectors)\n",
    "\n",
    "def get_cluster_features(text):\n",
    "    cluster_count = np.zeros(50)\n",
    "    if not isinstance(text, str) or not text.strip():  # Boş veya NaN string varsa\n",
    "        return cluster_count\n",
    "    for word in text.split():\n",
    "        if word in word_vectors:\n",
    "            try:\n",
    "                vec = np.asarray(word_vectors[word], dtype=np.float64)\n",
    "                idx = kmeans.predict([vec])[0]\n",
    "                cluster_count[idx] += 1\n",
    "            except Exception as e:\n",
    "                continue  # hata varsa geç\n",
    "    return cluster_count\n",
    "\n",
    "\n",
    "semantic_vectors = np.array([get_cluster_features(text) for text in df[\"clean_text\"]])\n",
    "semantic_vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((X_lsa, semantic_vectors))\n",
    "y = df[\"label\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm = SVC(kernel='linear', probability=True)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Modelleri kaydet\n",
    "pickle.dump(mlp, open(\"models/model_mlp.pkl\", \"wb\"))\n",
    "pickle.dump(rf, open(\"models/model_rf.pkl\", \"wb\"))\n",
    "pickle.dump(svm, open(\"models/model_svm.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MLP ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      4264\n",
      "           1       0.95      0.96      0.96      3520\n",
      "\n",
      "    accuracy                           0.96      7784\n",
      "   macro avg       0.96      0.96      0.96      7784\n",
      "weighted avg       0.96      0.96      0.96      7784\n",
      "\n",
      "F1 Score: 0.9559341123468956\n",
      "Accuracy: 0.9597893114080165\n",
      "ROC AUC: 0.9941875293151968\n",
      "\n",
      "--- Random Forest ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96      4264\n",
      "           1       0.95      0.94      0.95      3520\n",
      "\n",
      "    accuracy                           0.95      7784\n",
      "   macro avg       0.95      0.95      0.95      7784\n",
      "weighted avg       0.95      0.95      0.95      7784\n",
      "\n",
      "F1 Score: 0.9460651771737584\n",
      "Accuracy: 0.9513103802672148\n",
      "ROC AUC: 0.9901540913354938\n",
      "\n",
      "--- SVM ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      4264\n",
      "           1       0.95      0.92      0.94      3520\n",
      "\n",
      "    accuracy                           0.94      7784\n",
      "   macro avg       0.94      0.94      0.94      7784\n",
      "weighted avg       0.94      0.94      0.94      7784\n",
      "\n",
      "F1 Score: 0.9355581127733027\n",
      "Accuracy: 0.9424460431654677\n",
      "ROC AUC: 0.9862344496205014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "for model, name in zip([mlp, rf, svm], [\"MLP\", \"Random Forest\", \"SVM\"]):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
